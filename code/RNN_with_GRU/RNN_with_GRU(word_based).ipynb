{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "466 RNN with GRU(word-based).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbduK4oCsmTD"
      },
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "import spacy\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import spacy\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import  Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "file = open(\"/content/drive/MyDrive/466 data/en_tech_train.txt\", \"rb\")\n",
        "train_set = file.read().decode(encoding='utf-8')\n",
        "file = open(\"/content/drive/MyDrive/466 data/en_tech_test.txt\", \"rb\")\n",
        "test_set = file.read().decode(encoding='utf-8')\n",
        "file = open(\"/content/drive/MyDrive/466 data/en_tech_validate.txt\", \"rb\")\n",
        "validate_set = file.read().decode(encoding='utf-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lg5qWr3ooIZx",
        "outputId": "3bb13b96-8865-4f07-8d35-46c1c5e00217"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOOD5gWmvfBw"
      },
      "source": [
        "def predict_single(model, tokenizer, seed_text):\n",
        "  \n",
        "  predicted_word = None\n",
        "  encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "  encoded = pad_sequences([encoded], maxlen=5, truncating=\"pre\")\n",
        "\n",
        "  temp = model.predict(encoded)\n",
        "\n",
        "  y_predict = np.argmax(temp, axis=-1)\n",
        "\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "      if index == y_predict:\n",
        "        predicted_word = word\n",
        "        break\n",
        "  return str(predicted_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIaweIWX7FJU"
      },
      "source": [
        "def generate_text(model, tokenizer, text_seq_len, seed_text, n_words):\n",
        "  text = []\n",
        "  word_distribution = []\n",
        "\n",
        "  for _ in range(n_words):\n",
        "    encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    #encoded = pad_sequences([encoded], maxlen = 4, truncating='pre')\n",
        "\n",
        "    temp = model.predict(encoded)\n",
        "\n",
        "    y_predict = np.argmax(temp, axis=-1)\n",
        "    #print(y_predict)\n",
        "\n",
        "    word_list = []\n",
        "    max_index = y_predict\n",
        "    for x in range(4):\n",
        "      temp[0][max_index] = -1\n",
        "      max_index = np.argmax(temp, axis=-1)\n",
        "      word_list.append(max_index)\n",
        "\n",
        "    predicted_word = ''\n",
        "    temp = model.predict(encoded)\n",
        "\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == y_predict:\n",
        "        predicted_word = word\n",
        "        break\n",
        "    seed_text = seed_text + ' ' + predicted_word\n",
        "    text.append(predicted_word)\n",
        "\n",
        "    distribution = [(predicted_word, temp[0][y_predict][0])]\n",
        "    for predicted in word_list:\n",
        "      predicted_word = ''\n",
        "      for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "          predicted_word = word\n",
        "          break\n",
        "      distribution.append((predicted_word, temp[0][predicted][0]))\n",
        "    word_distribution.append(distribution)\n",
        "\n",
        "  return ' '.join(text), word_distribution\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxF8d2LD4U54"
      },
      "source": [
        "def validate(sentences, nlp):\n",
        "  new = []\n",
        "  for i in range(len(sentences)):\n",
        "    new.append(sentences[i].split())\n",
        "  validate_sequences = np.array(new)\n",
        "  validate_X, validate_y = validate_sequences[:, :-1], validate_sequences[:, -1]\n",
        "  average_accuracy = 0\n",
        "  num = 0\n",
        "  for i in range(len(validate_X)):\n",
        "    prediction = predict_single(model, tokenizer, ' '.join(validate_X[i]))\n",
        "    x = nlp(prediction)\n",
        "    y = nlp(str(validate_y[i]))\n",
        "\n",
        "    if (x[0].has_vector) and (y[0].has_vector):\n",
        "      average_accuracy += nlp(prediction).similarity(nlp(str(validate_y[i])))\n",
        "      num += 1\n",
        "\n",
        "  average_accuracy /= num\n",
        "  return average_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFI9h_JRLV-5"
      },
      "source": [
        "processed_train_set = train_set.split(\"\\n\")\n",
        "processed_test_set = test_set.split(\"\\n\")\n",
        "processed_validate_set = validate_set.split(\"\\n\")\n",
        "\n",
        "input_size = 6\n",
        "test_size = 6\n",
        "train_sentences = []\n",
        "test_sentences = []\n",
        "validate_sentences = []\n",
        "for line in processed_train_set:\n",
        "    temp = line.split()\n",
        "    for i in range(input_size, len(temp)):\n",
        "        seq = temp[i - input_size:i]\n",
        "        temp2 = \" \".join(seq)\n",
        "        train_sentences.append(temp2)\n",
        "#train_sentences[0:10]\n",
        "for line in processed_test_set:\n",
        "    temp = line.split()\n",
        "    for i in range(test_size, len(temp)):\n",
        "        seq = temp[i - test_size:i]\n",
        "        temp2 = \" \".join(seq)\n",
        "        test_sentences.append(temp2)\n",
        "print(test_sentences[0:10])\n",
        "for line in processed_validate_set:\n",
        "    temp = line.split()\n",
        "    for i in range(input_size, len(temp)):\n",
        "        seq = temp[i - input_size:i]\n",
        "        temp2 = \" \".join(seq)\n",
        "        validate_sentences.append(temp2)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "train_sequences = np.array(train_sequences)\n",
        "train_X, train_y = train_sequences[:, :-1], train_sequences[:, -1]\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "#model = tf.keras.models.load_model('/content/drive/MyDrive/model/saved_model/my_model')\n",
        "train_y = to_categorical(train_y, num_classes = vocab_size)\n",
        "# sequences = np.array(sequences)\n",
        "# print(sequences.shape)\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 256))\n",
        "# model.add(GRU(512, return_sequences=True))\n",
        "model.add(GRU(256))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(train_X,train_y, batch_size=400, epochs=40)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJLGT4MGXuRe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c2c3383-5ad7-40ac-d3c0-5cba3f1be0ff"
      },
      "source": [
        " accuracy = validate(test_sentences, nlp)\n",
        " print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5470152721009508\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}