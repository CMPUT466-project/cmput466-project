{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "466 RNN with GRU(word-based).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbduK4oCsmTD",
        "outputId": "b44aef06-429a-45f0-926b-844ae0036f17"
      },
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "import spacy\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import spacy\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import  Sequential\n",
        "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "file = open(\"/content/drive/MyDrive/466 data/en_tech_train.txt\", \"rb\")\n",
        "train_set = file.read().decode(encoding='utf-8')\n",
        "file = open(\"/content/drive/MyDrive/466 data/en_tech_test.txt\", \"rb\")\n",
        "test_set = file.read().decode(encoding='utf-8')\n",
        "file = open(\"/content/drive/MyDrive/466 data/en_tech_validate.txt\", \"rb\")\n",
        "validate_set = file.read().decode(encoding='utf-8')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_lg==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz#egg=en_core_web_lg==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (54.2.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOOD5gWmvfBw"
      },
      "source": [
        "def predict_single(model, tokenizer, seed_text):\n",
        "  \n",
        "  predicted_word = None\n",
        "  encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "  encoded = pad_sequences([encoded], maxlen = 4, truncating='pre')\n",
        "\n",
        "  temp = model.predict(encoded)\n",
        "\n",
        "  y_predict = np.argmax(temp, axis=-1)\n",
        "\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "      if index == y_predict:\n",
        "        predicted_word = word\n",
        "        break\n",
        "  return str(predicted_word)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIaweIWX7FJU"
      },
      "source": [
        "def generate_text(model, tokenizer, text_seq_len, seed_text, n_words):\n",
        "  text = []\n",
        "  word_distribution = []\n",
        "\n",
        "  for _ in range(n_words):\n",
        "    encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    #encoded = pad_sequences([encoded], maxlen = 4, truncating='pre')\n",
        "\n",
        "    temp = model.predict(encoded)\n",
        "\n",
        "    y_predict = np.argmax(temp, axis=-1)\n",
        "    #print(y_predict)\n",
        "\n",
        "    word_list = []\n",
        "    max_index = y_predict\n",
        "    for x in range(4):\n",
        "      temp[0][max_index] = -1\n",
        "      max_index = np.argmax(temp, axis=-1)\n",
        "      word_list.append(max_index)\n",
        "\n",
        "    predicted_word = ''\n",
        "    temp = model.predict(encoded)\n",
        "\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == y_predict:\n",
        "        predicted_word = word\n",
        "        break\n",
        "    seed_text = seed_text + ' ' + predicted_word\n",
        "    text.append(predicted_word)\n",
        "\n",
        "    distribution = [(predicted_word, temp[0][y_predict][0])]\n",
        "    for predicted in word_list:\n",
        "      predicted_word = ''\n",
        "      for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "          predicted_word = word\n",
        "          break\n",
        "      distribution.append((predicted_word, temp[0][predicted][0]))\n",
        "    word_distribution.append(distribution)\n",
        "\n",
        "  return ' '.join(text), word_distribution\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxF8d2LD4U54"
      },
      "source": [
        "def validate(validate_sentences, nlp):\n",
        "  new = []\n",
        "  for i in range(len(validate_sentences)):\n",
        "    new.append(validate_sentences[i].split())\n",
        "  validate_sequences = np.array(new)\n",
        "  validate_X, validate_y = validate_sequences[:, :-1], validate_sequences[:, -1]\n",
        "  average_accuracy = 0\n",
        "  num = 0\n",
        "  for i in range(len(validate_X)):\n",
        "    prediction = predict_single(model, tokenizer, ' '.join(validate_X[i]))\n",
        "    x = nlp(prediction)\n",
        "    y = nlp(str(validate_y[i]))\n",
        "\n",
        "    if (x[0].has_vector) and (y[0].has_vector):\n",
        "      average_accuracy += nlp(prediction).similarity(nlp(str(validate_y[i])))\n",
        "      num += 1\n",
        "\n",
        "  average_accuracy /= num\n",
        "  return average_accuracy"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFI9h_JRLV-5",
        "outputId": "b83fe64c-1b18-4c38-ec59-31c43e51ddf8"
      },
      "source": [
        "processed_train_set = train_set.split(\"\\n\")\n",
        "processed_test_set = test_set.split(\"\\n\")\n",
        "processed_validate_set = validate_set.split(\"\\n\")\n",
        "\n",
        "input_size = 5\n",
        "train_sentences = []\n",
        "test_sentences = []\n",
        "validate_sentences = []\n",
        "for line in processed_train_set:\n",
        "    temp = line.split()\n",
        "    for i in range(input_size, len(temp)):\n",
        "        seq = temp[i - input_size:i]\n",
        "        temp2 = \" \".join(seq)\n",
        "        train_sentences.append(temp2)\n",
        "train_sentences[0:10]\n",
        "for line in processed_test_set:\n",
        "    temp = line.split()\n",
        "    for i in range(input_size, len(temp)):\n",
        "        seq = temp[i - input_size:i]\n",
        "        temp2 = \" \".join(seq)\n",
        "        test_sentences.append(temp2)\n",
        "test_sentences[0:10]\n",
        "for line in processed_validate_set:\n",
        "    temp = line.split()\n",
        "    for i in range(input_size, len(temp)):\n",
        "        seq = temp[i - input_size:i]\n",
        "        temp2 = \" \".join(seq)\n",
        "        validate_sentences.append(temp2)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "train_sequences = np.array(train_sequences)\n",
        "train_X, train_y = train_sequences[:, :-1], train_sequences[:, -1]\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "train_y = to_categorical(train_y, num_classes = vocab_size)\n",
        "# sequences = np.array(sequences)\n",
        "# print(sequences.shape)\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 256))\n",
        "# model.add(GRU(512, return_sequences=True))\n",
        "model.add(GRU(512))\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.summary()\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(train_X,train_y, batch_size=256, epochs=20)\n",
        "accuracy = validate(validate_sentences, nlp)\n",
        "print(accuracy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, None, 256)         4601600   \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (None, 512)               1182720   \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 17975)             18424375  \n",
            "=================================================================\n",
            "Total params: 24,734,007\n",
            "Trainable params: 24,734,007\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "801/801 [==============================] - 36s 42ms/step - loss: 7.3767 - accuracy: 0.0700\n",
            "Epoch 2/20\n",
            "801/801 [==============================] - 34s 42ms/step - loss: 6.1322 - accuracy: 0.1378\n",
            "Epoch 3/20\n",
            "801/801 [==============================] - 34s 42ms/step - loss: 5.3699 - accuracy: 0.1773\n",
            "Epoch 4/20\n",
            "801/801 [==============================] - 34s 42ms/step - loss: 4.5262 - accuracy: 0.2308\n",
            "Epoch 5/20\n",
            "801/801 [==============================] - 34s 42ms/step - loss: 3.5987 - accuracy: 0.3133\n",
            "Epoch 6/20\n",
            "801/801 [==============================] - 34s 42ms/step - loss: 2.6853 - accuracy: 0.4399\n",
            "Epoch 7/20\n",
            "801/801 [==============================] - 34s 42ms/step - loss: 1.9873 - accuracy: 0.5606\n",
            "Epoch 8/20\n",
            "801/801 [==============================] - 34s 43ms/step - loss: 1.4605 - accuracy: 0.6597\n",
            "Epoch 9/20\n",
            "801/801 [==============================] - 33s 42ms/step - loss: 1.0680 - accuracy: 0.7418\n",
            "Epoch 10/20\n",
            "801/801 [==============================] - 33s 41ms/step - loss: 0.7720 - accuracy: 0.8108\n",
            "Epoch 11/20\n",
            "801/801 [==============================] - 33s 41ms/step - loss: 0.5610 - accuracy: 0.8628\n",
            "Epoch 12/20\n",
            "801/801 [==============================] - 33s 41ms/step - loss: 0.4180 - accuracy: 0.8988\n",
            "Epoch 13/20\n",
            "801/801 [==============================] - 33s 41ms/step - loss: 0.3138 - accuracy: 0.9272\n",
            "Epoch 14/20\n",
            "801/801 [==============================] - 33s 42ms/step - loss: 0.2558 - accuracy: 0.9422\n",
            "Epoch 15/20\n",
            "801/801 [==============================] - 34s 42ms/step - loss: 0.2274 - accuracy: 0.9487\n",
            "Epoch 16/20\n",
            "801/801 [==============================] - 34s 42ms/step - loss: 0.2040 - accuracy: 0.9550\n",
            "Epoch 17/20\n",
            "801/801 [==============================] - 34s 42ms/step - loss: 0.1913 - accuracy: 0.9562\n",
            "Epoch 18/20\n",
            "801/801 [==============================] - 34s 42ms/step - loss: 0.1853 - accuracy: 0.9564\n",
            "Epoch 19/20\n",
            "801/801 [==============================] - 34s 42ms/step - loss: 0.1749 - accuracy: 0.9580\n",
            "Epoch 20/20\n",
            "801/801 [==============================] - 34s 42ms/step - loss: 0.1643 - accuracy: 0.9600\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}